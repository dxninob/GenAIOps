# ğŸ¤– Chatbot GenAI - Profesor de EstadÃ­stica

Este proyecto demuestra cÃ³mo construir, evaluar y automatizar un chatbot de tipo RAG (Retrieval Augmented Generation) con buenas prÃ¡cticas de **GenAIOps**.

---

## ğŸ§  Caso de Estudio

El chatbot responde preguntas sobre estadistÃ­ca bÃ¡sica y probabilidad, estÃ¡ enfocado especificamente para resolver dudas de esta materia a los estudientes de primeros semestres de pregrado. El chatbot usa como base una colecciÃ³n de documentos PDF de libros de estadÃ­stica universitaria bÃ¡sica.

---

## ğŸ“‚ Estructura del Proyecto

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ ui_streamlit.py           â† interfaz simple del chatbot
â”‚   â”œâ”€â”€ run_eval.py               â† evaluaciÃ³n automÃ¡tica
â”‚   â”œâ”€â”€ rag_pipeline.py           â† lÃ³gica de ingestiÃ³n y RAG
â”‚   â”œâ”€â”€ dashboard.py              â† interfaz de metricas evaluadas
â”‚   â””â”€â”€ prompts/
â”‚       â”œâ”€â”€ v1_profesor_estadistica.txt
â”‚       â”œâ”€â”€ v2_resumido_directo.txt
â”‚       â””â”€â”€ v3_profesor_primaria.txt
â”œâ”€â”€ data/pdfs/                    â† documentos fuente
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_run_eval.py
â”‚   â”œâ”€â”€ eval_dataset.json         â† dataset de evaluaciÃ³n
â”‚   â””â”€â”€ eval_dataset.csv
â”œâ”€â”€ .env.example
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .devcontainer/
â”‚   â””â”€â”€ devcontainer.json
â”œâ”€â”€ .github/workflows/
â”‚   â”œâ”€â”€ eval.yml
â”‚   â””â”€â”€ test.yml
```

---

## ğŸš¦ Ciclo de vida GenAIOps aplicado

### 1. ğŸ§± PreparaciÃ³n del entorno

```bash
git clone https://github.com/darkanita/GenAIOps_Pycon2025 chatbot-genaiops
cd chatbot-genaiops
conda create -n chatbot-genaiops python=3.10 -y
conda activate chatbot-genaiops
pip install -r requirements.txt
cp .env.example .env  # Agrega tu API KEY de OpenAI
```

---

### 2. ğŸ” Ingesta y vectorizaciÃ³n de documentos

Procesa los PDFs y genera el Ã­ndice vectorial:

```bash
python -c "from app.rag_pipeline import save_vectorstore; save_vectorstore()"
```

Esto:
- Divide los documentos en chunks (por defecto `chunk_size=512`, `chunk_overlap=50`)
- Genera embeddings con OpenAI
- Guarda el Ã­ndice vectorial en `vectorstore/`
- Registra los parÃ¡metros en **MLflow**

ğŸ”§ Para personalizar:
```python
save_vectorstore(chunk_size=1024, chunk_overlap=100)
```

â™»ï¸ Para reutilizarlo directamente:
```python
vectordb = load_vectorstore_from_disk()
```

---

### 3. ğŸ§  ConstrucciÃ³n del pipeline RAG

```python
from app.rag_pipeline import build_chain
chain = build_chain(vectordb, prompt_version="v1_profesor_estadistica")
```

- Soporta mÃºltiples versiones de prompt
- Usa `ConversationalRetrievalChain` con `LangChain` + `OpenAI`

---

### 4. ğŸ’¬ InteracciÃ³n vÃ­a Streamlit

VersiÃ³n bÃ¡sica:
```bash
streamlit run app/ui_streamlit.py
```

---

### 5. ğŸ§ª EvaluaciÃ³n automÃ¡tica de calidad

Ejecuta:

```bash
python app/run_eval.py
```

Esto:
- Usa `tests/eval_dataset.json` como ground truth
- Genera respuestas usando el RAG actual
- EvalÃºa con `LangChain Evaluation (load_evaluator)`
- Registra resultados en **MLflow**

---

### 6. ğŸ“ˆ VisualizaciÃ³n de resultados

Dashboard completo:

```bash
streamlit run app/dashboard.py
```

- Tabla con todas las preguntas evaluadas
- GrÃ¡ficos de precisiÃ³n por configuraciÃ³n (`prompt + chunk_size`)
- Filtrado por experimento MLflow

---

### 7. ğŸ” AutomatizaciÃ³n con GitHub Actions

- CI de evaluaciÃ³n: `.github/workflows/eval.yml`
- Test unitarios: `.github/workflows/test.yml`

---

### 8. ğŸ§ª ValidaciÃ³n automatizada

```bash
pytest tests/test_run_eval.py
```

- EvalÃºa que el sistema tenga al menos 80% de precisiÃ³n con el dataset base

---

## ğŸ” Â¿QuÃ© puedes hacer?

- ğŸ’¬ Hacer preguntas al chatbot
- ğŸ” Evaluar diferentes estrategias de chunking y prompts
- ğŸ“Š Comparar desempeÃ±o con mÃ©tricas semÃ¡nticas
- ğŸ§ª Trazar todo en MLflow
- ğŸ”„ Adaptar a otros dominios (legal, salud, educaciÃ³nâ€¦)

---

## âš™ï¸ Stack TecnolÃ³gico

- **OpenAI + LangChain** â€“ LLM + RAG
- **FAISS** â€“ Vectorstore
- **Streamlit** â€“ UI
- **MLflow** â€“ Registro de experimentos
- **LangChain Eval** â€“ EvaluaciÃ³n semÃ¡ntica
- **GitHub Actions** â€“ CI/CD
- **DevContainer** â€“ Desarrollo portable

---

## ğŸ“ DesafÃ­o para estudiantes

ğŸ§© **Parte 1: PersonalizaciÃ³n**

**1. SelecciÃ³n de un nuevo dominio**  
Se seleccionÃ³ el dominio de estadÃ­stica bÃ¡sica y probabilidad.

**2. Reemplazo de los documentos PDF**  
Se adjuntÃ³ en la ruta data/pdfs/ un total de cinco libros relacionados con el tema.

**3. CreaciÃ³n de prompts**  
Se adjuntÃ³ el la ruta app/prompts/ un total de tres prompts para ser evaluados.
- Profesor universitario (v1_profesor_estadistica): Responde como un profesor universitario en estadÃ­stica y probabilidad que da esta clase a estudiantes de primeros semestres. Responde Ãºnicamente usando los libros en PDF y si no sabe la respuesta admite que no tiene suficiente informaciÃ³n. 
- Resumido directo (v2_resumido_directo): Responde de forma breve y directa, usando Ãºnicamente los libros en PDF y si no sabe la respuesta admite que no tiene suficiente informaciÃ³n. 
- Profesor de primaria (v3_profesor_primaria): Responde como un profesor de primaria de estadistica y probabilidad que da clase a niÃ±os entre 6 y 10 aÃ±os. Responde de forma sencilla y sin terminos tecnicos. No responde necesariamente usando los libros en PDF pero los puede usar para complementar su respuesta. No admite que no tiene suficiente informaciÃ³n. 

**4. CreaciÃ³n de un conjunto de pruebas**  
En tests/eval_dataset.json, se definieron 21 preguntas junto con su respuesta esperada para evaluar al chatbot.

ğŸ”§ **Parte 2: Reto**

**1. Mejoramiento del sistema de evaluaciÃ³n:**
- Se evaluÃ³ el conjunto de pruebas usando los siguientes criterios:
  * "correctness" â€“ Â¿Es correcta la respuesta?
  * "relevance" â€“ Â¿Es relevante respecto a la pregunta?
  * "coherence" â€“ Â¿EstÃ¡ bien estructurada la respuesta?
  * "toxicity" â€“ Â¿Contiene lenguaje ofensivo o riesgoso?
  * "harmfulness" â€“ Â¿PodrÃ­a causar daÃ±o la informaciÃ³n?

- Para cada criterio se registrÃ³ una mÃ©trica en MLflow (score)

![Imagen](/static/Experimento_individual.png)  

ğŸ“Š **Parte 3: Mejora del dashboard**

**1. AgregaciÃ³n de metricas para visualizar en dashboard.py:**  
Se agregÃ³ al dashboard:
- Las mÃ©tricas por criterio (correctness_score, toxicity_score, etc.).  

![Imagen](/static/DesempeÃ±o_individual.png)

- Una opciÃ³n para seleccionar y comparar diferentes criterios en grÃ¡ficos (las imagenes mostradas en la siguiente secciÃ³n).

ğŸ§ª **Parte 5: Presenta y reflexiona**
**1. Compara configuraciones distintas (chunk size, prompt) y justifica tu selecciÃ³n.**  
Se evaluaron los tres prompts usando dos vectorstores (CHUNK_SIZE=512,CHUNK_OVERLAP=50; CHUNK_SIZE=256,CHUNK_OVERLAP=30). Por lo tanto en total hubo seis configuraciones diferentes. En MLflow se registraron los seis experimentos de cada configuraciÃ³n:  
![Imagen](/static/Experimentos.png)  

- La configuraciÃ³n que genera mejores respuestas es la del prompt del profesor universitario (v1_profesor_estadistica) con chuncking mÃ¡s grande (CHUNK_SIZE=512 y CHUNK_OVERLAP=50). Esto tiene sentido ya que ese prompt es el que da un contexto mÃ¡s acorde para obtener las respuestas esperadas de las preguntas de testing, ademÃ¡s porque al ser mÃ¡s grande el chunking el modelo puede tener mÃ¡s contexto para responder de mejor forma.

![Imagen](/static/Comparacion_criterios.png)  

- ComparaciÃ³n entre prompts: para ambas configuraciones de chuncking, el prompt que obtuvo mejores resultados fue el del profesor universitario, seguido del profesor de primaria, y el de peores resultados fue el prompt de resumido directo. El resumido directo pudo dar peores metricas dado que es el prompt menos especifico, era el que mÃ¡s libertad le daba al modelo por lo cual pudo haber dado peor informaciÃ³n.

![Imagen](/static/Comparacion_promts.png)  

- ComparaciÃ³n entre chuncking: el tamaÃ±o del chuncking no generÃ³ diferencias muy significativas entre las respuestas para los prompts de profesor universitario y profesor de primaria, pero en el caso de resumido directo sÃ­ afectÃ³ mucho la respuesta para todos los criterios. En general, el chunking mÃ¡s grande fue el que logrÃ³ dar mejores metricas dado que da mÃ¡s contexto al modelo. 

![Imagen](/static/Comparacion_chunking.png)
